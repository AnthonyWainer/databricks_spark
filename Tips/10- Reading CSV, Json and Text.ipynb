{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"test\").master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+-----+----------+-----------+--------+\n",
      "| ID|               Name|  Fee|     Venue|       Date|Duration|\n",
      "+---+-------------------+-----+----------+-----------+--------+\n",
      "|  1|          HE Hadoop| 9000|    Mumbai|01-Aug-2018|       2|\n",
      "|  2|           HE Spark| 7000|   Kolkata|04-Aug-2018|       3|\n",
      "|  3|        HE SparkSQL| 6000| Hyderabad|07-Aug-2018|       4|\n",
      "|  4|     HE Spark Graph| 6000|   Chennai|10-Aug-2018|       5|\n",
      "|  5|HE Machine Learning|10000|    London|13-Aug-2018|       2|\n",
      "|  6|    HE Data Science|12000|Washington|16-Aug-2018|       3|\n",
      "|  7|            HE Java| 6000|    Navada|19-Aug-2018|       4|\n",
      "|  8|           HE Scala| 6000|   Newyork|22-Aug-2018|       5|\n",
      "|  9|          HE Python| 6000|    Sydney|25-Aug-2018|       2|\n",
      "| 10|            HE Unix| 7000| Singapore|28-Aug-2018|       3|\n",
      "| 11|             HE C++| 5000|      Pune|31-Aug-2018|       4|\n",
      "| 12|  HE Data Structure| 4000| New Delhi|03-Sep-2018|       5|\n",
      "| 13|     HE Web Service| 6000|  Yokohama|06-Sep-2018|       2|\n",
      "| 14|HE Spring Framework| 8000|     Osaka|09-Sep-2018|       3|\n",
      "| 15|       HE Hybernate| 7000|   Beijing|12-Sep-2018|       4|\n",
      "| 16|             HE JPA| 7000| Hong Kong|15-Sep-2018|       5|\n",
      "| 17|     HE Silverlight| 6000|  Shenzhen|18-Sep-2018|       2|\n",
      "| 18|      HE Oracle DBA|10000|    Berlin|21-Sep-2018|       3|\n",
      "| 19|    HE Oracle PLSQL| 6000|    Munich|24-Sep-2018|       4|\n",
      "| 20|       HE Cassandra|10000| Frankfurt|27-Sep-2018|       5|\n",
      "+---+-------------------+-----+----------+-----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#format agnostic load method\n",
    "spark.read.format(\"csv\").option(\"header\", True).option(\"inferschema\", True)\\\n",
    ".load(\"../DataDummy/data/DataBricks_Training.csv\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+---+------+---------+\n",
      "|duration| fee| id|  name|    venue|\n",
      "+--------+----+---+------+---------+\n",
      "|       5|6000|  1|Hadoop|     Lima|\n",
      "|       4|5000|  2| Spark|     Puno|\n",
      "|       3|4000|  3|Python|Cajamarca|\n",
      "|       3|4000|  4| Scala| Tarapoto|\n",
      "|       7|7000|  5| HBase| Arequipa|\n",
      "+--------+----+---+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"json\").option(\"header\", True)\\\n",
    ".option(\"inferSchema\", True).load(\"../DataDummy/data/he_data_1.json\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading CSV file From Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|ID,Name,Fee,Venue...|\n",
      "|1,HE Hadoop,9000,...|\n",
      "|2,HE Spark,7000,K...|\n",
      "|3,HE SparkSQL,600...|\n",
      "|4,HE Spark Graph,...|\n",
      "|5,HE Machine Lear...|\n",
      "|6,HE Data Science...|\n",
      "|7,HE Java,6000,Na...|\n",
      "|8,HE Scala,6000,N...|\n",
      "|9,HE Python,6000,...|\n",
      "|10,HE Unix,7000,S...|\n",
      "|11,HE C++,5000,Pu...|\n",
      "|12,HE Data Struct...|\n",
      "|13,HE Web Service...|\n",
      "|14,HE Spring Fram...|\n",
      "|15,HE Hybernate,7...|\n",
      "|16,HE JPA,7000,Ho...|\n",
      "|17,HE Silverlight...|\n",
      "|18,HE Oracle DBA,...|\n",
      "|19,HE Oracle PLSQ...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.text(\"../DataDummy/data/DataBricks_Training.csv\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------+\n",
      "|value                                                   |\n",
      "+--------------------------------------------------------+\n",
      "|[ID, Name, Fee, Venue, Date, Duration]                  |\n",
      "|[1, HE Hadoop, 9000, Mumbai, 01-Aug-2018, 2]            |\n",
      "|[2, HE Spark, 7000, Kolkata, 04-Aug-2018, 3]            |\n",
      "|[3, HE SparkSQL, 6000, Hyderabad, 07-Aug-2018, 4]       |\n",
      "|[4, HE Spark Graph, 6000, Chennai, 10-Aug-2018, 5]      |\n",
      "|[5, HE Machine Learning, 10000, London, 13-Aug-2018, 2] |\n",
      "|[6, HE Data Science, 12000, Washington, 16-Aug-2018, 3] |\n",
      "|[7, HE Java, 6000, Navada, 19-Aug-2018, 4]              |\n",
      "|[8, HE Scala, 6000, Newyork, 22-Aug-2018, 5]            |\n",
      "|[9, HE Python, 6000, Sydney, 25-Aug-2018, 2]            |\n",
      "|[10, HE Unix, 7000, Singapore, 28-Aug-2018, 3]          |\n",
      "|[11, HE C++, 5000, Pune, 31-Aug-2018, 4]                |\n",
      "|[12, HE Data Structure, 4000, New Delhi, 03-Sep-2018, 5]|\n",
      "|[13, HE Web Service, 6000, Yokohama, 06-Sep-2018, 2]    |\n",
      "|[14, HE Spring Framework, 8000, Osaka, 09-Sep-2018, 3]  |\n",
      "|[15, HE Hybernate, 7000, Beijing, 12-Sep-2018, 4]       |\n",
      "|[16, HE JPA, 7000, Hong Kong, 15-Sep-2018, 5]           |\n",
      "|[17, HE Silverlight, 6000, Shenzhen, 18-Sep-2018, 2]    |\n",
      "|[18, HE Oracle DBA, 10000, Berlin, 21-Sep-2018, 3]      |\n",
      "|[19, HE Oracle PLSQL, 6000, Munich, 24-Sep-2018, 4]     |\n",
      "+--------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"value\", split(\"value\", \",\")).show(20, False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

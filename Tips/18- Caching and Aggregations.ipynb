{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row, SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"test\").master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\").option(\"header\", True).option(\"inferSchema\", True).load(\"../DataDummy/data/DataBricks_Training.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#store in cache\n",
    "spark.sql(\"CACHE TABLE test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check store cache\n",
    "spark.catalog.isCached(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#clear cache\n",
    "spark.sql(\"CLEAR CACHE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check store cache\n",
    "spark.catalog.isCached(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STORAGELEVEL\n",
    "from pyspark import StorageLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.cacheTable(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+-----+----------+-----------+--------+\n",
      "| ID|               Name|  Fee|     Venue|       Date|Duration|\n",
      "+---+-------------------+-----+----------+-----------+--------+\n",
      "|  1|          HE Hadoop| 9000|    Mumbai|01-Aug-2018|       2|\n",
      "|  2|           HE Spark| 7000|   Kolkata|04-Aug-2018|       3|\n",
      "|  3|        HE SparkSQL| 6000| Hyderabad|07-Aug-2018|       4|\n",
      "|  4|     HE Spark Graph| 6000|   Chennai|10-Aug-2018|       5|\n",
      "|  5|HE Machine Learning|10000|    London|13-Aug-2018|       2|\n",
      "|  6|    HE Data Science|12000|Washington|16-Aug-2018|       3|\n",
      "|  7|            HE Java| 6000|    Navada|19-Aug-2018|       4|\n",
      "|  8|           HE Scala| 6000|   Newyork|22-Aug-2018|       5|\n",
      "|  9|          HE Python| 6000|    Sydney|25-Aug-2018|       2|\n",
      "| 10|            HE Unix| 7000| Singapore|28-Aug-2018|       3|\n",
      "| 11|             HE C++| 5000|      Pune|31-Aug-2018|       4|\n",
      "| 12|  HE Data Structure| 4000| New Delhi|03-Sep-2018|       5|\n",
      "| 13|     HE Web Service| 6000|  Yokohama|06-Sep-2018|       2|\n",
      "| 14|HE Spring Framework| 8000|     Osaka|09-Sep-2018|       3|\n",
      "| 15|       HE Hybernate| 7000|   Beijing|12-Sep-2018|       4|\n",
      "| 16|             HE JPA| 7000| Hong Kong|15-Sep-2018|       5|\n",
      "| 17|     HE Silverlight| 6000|  Shenzhen|18-Sep-2018|       2|\n",
      "| 18|      HE Oracle DBA|10000|    Berlin|21-Sep-2018|       3|\n",
      "| 19|    HE Oracle PLSQL| 6000|    Munich|24-Sep-2018|       4|\n",
      "| 20|       HE Cassandra|10000| Frankfurt|27-Sep-2018|       5|\n",
      "+---+-------------------+-----+----------+-----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM test\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CLEAR CACHE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|  FEE|     VENUE|\n",
      "+-----+----------+\n",
      "|46000| Singapore|\n",
      "|37000| Frankfurt|\n",
      "|50000|   Beijing|\n",
      "|34000|    Navada|\n",
      "|44000|   Chennai|\n",
      "|29000|    Berlin|\n",
      "|49000|    London|\n",
      "|36000|    Mumbai|\n",
      "|42000|    Sydney|\n",
      "|37000|   Kolkata|\n",
      "|46000|Washington|\n",
      "|44000|   Newyork|\n",
      "|51000| Hong Kong|\n",
      "|46000|  Yokohama|\n",
      "|36000|      Pune|\n",
      "|39000| New Delhi|\n",
      "|43000|     Osaka|\n",
      "|36000|  Shenzhen|\n",
      "|41000| Hyderabad|\n",
      "|41000|     Dubai|\n",
      "+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT SUM(FEE) as FEE, VENUE FROM TEST GROUP BY VENUE\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|total_fee|\n",
      "+---------+\n",
      "|   855000|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.agg(sum(\"fee\").alias(\"total_fee\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|     venue|total_fee|\n",
      "+----------+---------+\n",
      "| Singapore|    46000|\n",
      "| Frankfurt|    37000|\n",
      "|   Beijing|    50000|\n",
      "|    Navada|    34000|\n",
      "|   Chennai|    44000|\n",
      "|    Berlin|    29000|\n",
      "|    London|    49000|\n",
      "|    Mumbai|    36000|\n",
      "|    Sydney|    42000|\n",
      "|   Kolkata|    37000|\n",
      "|Washington|    46000|\n",
      "|   Newyork|    44000|\n",
      "| Hong Kong|    51000|\n",
      "|  Yokohama|    46000|\n",
      "|      Pune|    36000|\n",
      "| New Delhi|    39000|\n",
      "|     Osaka|    43000|\n",
      "|  Shenzhen|    36000|\n",
      "| Hyderabad|    41000|\n",
      "|     Dubai|    41000|\n",
      "+----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"venue\").agg(sum(\"fee\").alias(\"total_fee\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+\n",
      "|     venue|average|\n",
      "+----------+-------+\n",
      "| Singapore| 9200.0|\n",
      "| Frankfurt| 9250.0|\n",
      "|   Beijing|10000.0|\n",
      "|    Navada| 6800.0|\n",
      "|   Chennai| 8800.0|\n",
      "|    Berlin| 7250.0|\n",
      "|    London| 9800.0|\n",
      "|    Mumbai| 7200.0|\n",
      "|    Sydney| 8400.0|\n",
      "|   Kolkata| 7400.0|\n",
      "|Washington| 9200.0|\n",
      "|   Newyork| 8800.0|\n",
      "| Hong Kong|10200.0|\n",
      "|  Yokohama| 9200.0|\n",
      "|      Pune| 7200.0|\n",
      "| New Delhi| 7800.0|\n",
      "|     Osaka| 8600.0|\n",
      "|  Shenzhen| 9000.0|\n",
      "| Hyderabad| 8200.0|\n",
      "|     Dubai|10250.0|\n",
      "+----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"venue\").agg(avg(\"fee\").alias(\"average\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import max, min, count, avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+----------+----------+--------+------------+\n",
      "|  fee| total|max(venue)|min(venue)|avg(fee)|count(venue)|\n",
      "+-----+------+----------+----------+--------+------------+\n",
      "| 4000|  8000|     Osaka| New Delhi|  4000.0|           2|\n",
      "|12000|228000|  Yokohama|   Beijing| 12000.0|          19|\n",
      "| 5000|  5000|      Pune|      Pune|  5000.0|           1|\n",
      "|10000|150000|  Yokohama|    Berlin| 10000.0|          15|\n",
      "|15000| 30000|     Osaka| Hong Kong| 15000.0|           2|\n",
      "| 6000|114000|  Yokohama|    Berlin|  6000.0|          19|\n",
      "| 9000| 36000|    Mumbai|     Dubai|  9000.0|           4|\n",
      "| 8000|144000|  Yokohama|   Chennai|  8000.0|          18|\n",
      "| 7000|140000|Washington|   Beijing|  7000.0|          20|\n",
      "+-----+------+----------+----------+--------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"fee\").agg(sum(\"fee\").alias(\"total\"), max(\"venue\"), min(\"venue\"), avg(\"fee\"), count(\"venue\")).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

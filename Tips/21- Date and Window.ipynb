{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"test\").master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|                City|Hadoop|\n",
      "+--------------------+------+\n",
      "| [Mumbai, Hyderabad]|  6000|\n",
      "|[NewYork, Washing...|  7000|\n",
      "|    [Sidney, London]|  8000|\n",
      "|   [Kolkata, Jaipur]|  9000|\n",
      "+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_df = spark.read.json(\"../DataDummy/data/HE_TRAINING.json\")\n",
    "json_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|      City|Hadoop|\n",
      "+----------+------+\n",
      "|    Mumbai|  6000|\n",
      "| Hyderabad|  6000|\n",
      "|   NewYork|  7000|\n",
      "|Washington|  7000|\n",
      "|    Sidney|  8000|\n",
      "|    London|  8000|\n",
      "|   Kolkata|  9000|\n",
      "|    Jaipur|  9000|\n",
      "+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_df.withColumn(\"City\", explode(\"City\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Date Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format, current_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+\n",
      "| id|          date_time|\n",
      "+---+-------------------+\n",
      "|  0|29-01-2020 04:42:45|\n",
      "+---+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(1).select(\"id\", date_format(current_timestamp(), \"dd-MM-yyyy hh:mm:ss\").alias(\"date_time\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import unix_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+\n",
      "| id| date_time|\n",
      "+---+----------+\n",
      "|  0|1580334355|\n",
      "+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(1).select(\"id\", unix_timestamp().alias(\"date_time\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit, to_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      date|\n",
      "+----------+\n",
      "|2020-01-29|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(1).select(to_date(lit(\"2020-01-29\")).alias(\"date\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row \n",
    "\n",
    "dates = spark.sparkContext.parallelize(\n",
    "    [ Row(1, \"2018-01-01\", 20000), \n",
    "     Row(1, \"2018-01-02\", 23000), \n",
    "     Row(1, \"2018-01-03\", 90000), \n",
    "     Row(1, \"2018-01-04\", 55000), \n",
    "     Row(1, \"2018-01-05\", 20000), \n",
    "     Row(1, \"2018-01-06\", 23000), \n",
    "     Row(1, \"2018-01-07\", 90000), \n",
    "     Row(1, \"2018-01-08\", 55000), \n",
    "     Row(2, \"2018-01-01\", 80000), \n",
    "     Row(2, \"2018-01-02\", 90000), \n",
    "     Row(2, \"2018-01-03\", 100000), \n",
    "     Row(2, \"2018-01-04\", 80000), \n",
    "     Row(2, \"2018-01-05\", 90000), \n",
    "     Row(2, \"2018-01-06\", 100000), \n",
    "     Row(2, \"2018-01-07\", 80000), \n",
    "     Row(2, \"2018-01-08\", 90000) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- fee: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "date_df = spark.createDataFrame(dates, [\"id\", \"date\", \"fee\"])\n",
    "date_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- fee: long (nullable = true)\n",
      " |-- date_cast: date (nullable = true)\n",
      "\n",
      "+---+----------+------+----------+\n",
      "| id|      date|   fee| date_cast|\n",
      "+---+----------+------+----------+\n",
      "|  1|2018-01-01| 20000|2018-01-01|\n",
      "|  1|2018-01-02| 23000|2018-01-02|\n",
      "|  1|2018-01-03| 90000|2018-01-03|\n",
      "|  1|2018-01-04| 55000|2018-01-04|\n",
      "|  1|2018-01-05| 20000|2018-01-05|\n",
      "|  1|2018-01-06| 23000|2018-01-06|\n",
      "|  1|2018-01-07| 90000|2018-01-07|\n",
      "|  1|2018-01-08| 55000|2018-01-08|\n",
      "|  2|2018-01-01| 80000|2018-01-01|\n",
      "|  2|2018-01-02| 90000|2018-01-02|\n",
      "|  2|2018-01-03|100000|2018-01-03|\n",
      "|  2|2018-01-04| 80000|2018-01-04|\n",
      "|  2|2018-01-05| 90000|2018-01-05|\n",
      "|  2|2018-01-06|100000|2018-01-06|\n",
      "|  2|2018-01-07| 80000|2018-01-07|\n",
      "|  2|2018-01-08| 90000|2018-01-08|\n",
      "+---+----------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "date_df = date_df.withColumn(\"date_cast\", col(\"date\").cast(\"date\"))\n",
    "\n",
    "date_df.printSchema()\n",
    "date_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import window,sum "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+---------+\n",
      "|              start|                end|total_fee|\n",
      "+-------------------+-------------------+---------+\n",
      "|2017-12-31 19:00:00|2018-01-01 19:00:00|   100000|\n",
      "|2018-01-01 19:00:00|2018-01-02 19:00:00|   113000|\n",
      "|2018-01-02 19:00:00|2018-01-03 19:00:00|   190000|\n",
      "|2018-01-03 19:00:00|2018-01-04 19:00:00|   135000|\n",
      "|2018-01-04 19:00:00|2018-01-05 19:00:00|   110000|\n",
      "|2018-01-05 19:00:00|2018-01-06 19:00:00|   123000|\n",
      "|2018-01-06 19:00:00|2018-01-07 19:00:00|   170000|\n",
      "|2018-01-07 19:00:00|2018-01-08 19:00:00|   145000|\n",
      "+-------------------+-------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# calculating the total fee every day across courses\n",
    "\n",
    "totalFeeEveryDay = date_df.groupBy(window(\"date_cast\", \"1 days\")).agg(sum(\"fee\").alias(\"total_fee\"))\\\n",
    ".select(\"window.start\", \"window.end\", \"total_fee\") \n",
    "\n",
    "totalFeeEveryDay.orderBy(\"start\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+---------+\n",
      "|              start|                end|total_fee|\n",
      "+-------------------+-------------------+---------+\n",
      "|2018-01-04 19:00:00|2018-01-06 19:00:00|   233000|\n",
      "|2018-01-06 19:00:00|2018-01-08 19:00:00|   315000|\n",
      "|2018-01-02 19:00:00|2018-01-04 19:00:00|   325000|\n",
      "|2017-12-31 19:00:00|2018-01-02 19:00:00|   213000|\n",
      "+-------------------+-------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Total fee collected in every two day\n",
    "date_df.groupBy(window(\"date_cast\", \"2 days\")).agg(sum(\"fee\").alias(\"total_fee\"))\\\n",
    ".select(\"window.start\", \"window.end\", \"total_fee\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

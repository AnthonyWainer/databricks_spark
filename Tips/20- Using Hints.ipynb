{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"test\").master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df = spark.read.format(\"csv\").option(\"inferschema\", True).option(\"header\", True)\\\n",
    ".load(\"DataDummy/data/DataBricks_Training.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----+-------+-----------+--------+\n",
      "| ID|     Name| Fee|  Venue|       Date|Duration|\n",
      "+---+---------+----+-------+-----------+--------+\n",
      "|  1|HE Hadoop|9000| Mumbai|01-Aug-2018|       2|\n",
      "|  2| HE Spark|7000|Kolkata|04-Aug-2018|       3|\n",
      "+---+---------+----+-------+-----------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_double_df = spark.read.format(\"csv\").option(\"sep\", \"|\").option(\"inferschema\", True).option(\"header\", True)\\\n",
    ".load(\"DataDummy/dataframe_sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+-------------------+--------+\n",
      "| id|           end_date|         start_date|location|\n",
      "+---+-------------------+-------------------+--------+\n",
      "|  1|2015-10-14 00:00:00|2015-09-14 00:00:00|   CA-SF|\n",
      "|  2|2015-10-15 01:00:20|2015-08-14 00:00:00|   CA-SD|\n",
      "+---+-------------------+-------------------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_double_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+-----+---------+-----------+--------+-------------------+-------------------+--------+\n",
      "| ID|               Name|  Fee|    Venue|       Date|Duration|           end_date|         start_date|location|\n",
      "+---+-------------------+-----+---------+-----------+--------+-------------------+-------------------+--------+\n",
      "|  1|          HE Hadoop| 9000|   Mumbai|01-Aug-2018|       2|2015-10-14 00:00:00|2015-09-14 00:00:00|   CA-SF|\n",
      "|  2|           HE Spark| 7000|  Kolkata|04-Aug-2018|       3|2015-10-15 01:00:20|2015-08-14 00:00:00|   CA-SD|\n",
      "|  3|        HE SparkSQL| 6000|Hyderabad|07-Aug-2018|       4|2015-10-16 02:30:00|2015-01-14 00:00:00|   NY-NY|\n",
      "|  4|     HE Spark Graph| 6000|  Chennai|10-Aug-2018|       5|2015-10-17 03:00:20|2015-02-14 00:00:00|   NY-NY|\n",
      "|  5|HE Machine Learning|10000|   London|13-Aug-2018|       2|2015-10-18 04:30:00|2014-04-14 00:00:00|   CA-SD|\n",
      "+---+-------------------+-----+---------+-----------+--------+-------------------+-------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_df.join(training_double_df.hint(\"id\"), \"ID\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Join UsingJoin(Inner,Buffer(ID))\n",
      ":- Relation[ID#57,Name#58,Fee#59,Venue#60,Date#61,Duration#62] csv\n",
      "+- Relation[id#193,end_date#194,start_date#195,location#196] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "ID: int, Name: string, Fee: int, Venue: string, Date: string, Duration: int, end_date: timestamp, start_date: timestamp, location: string\n",
      "Project [ID#57, Name#58, Fee#59, Venue#60, Date#61, Duration#62, end_date#194, start_date#195, location#196]\n",
      "+- Join Inner, (ID#57 = id#193)\n",
      "   :- Relation[ID#57,Name#58,Fee#59,Venue#60,Date#61,Duration#62] csv\n",
      "   +- Relation[id#193,end_date#194,start_date#195,location#196] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [ID#57, Name#58, Fee#59, Venue#60, Date#61, Duration#62, end_date#194, start_date#195, location#196]\n",
      "+- Join Inner, (ID#57 = id#193)\n",
      "   :- Filter isnotnull(ID#57)\n",
      "   :  +- Relation[ID#57,Name#58,Fee#59,Venue#60,Date#61,Duration#62] csv\n",
      "   +- Filter isnotnull(id#193)\n",
      "      +- Relation[id#193,end_date#194,start_date#195,location#196] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "*(2) Project [ID#57, Name#58, Fee#59, Venue#60, Date#61, Duration#62, end_date#194, start_date#195, location#196]\n",
      "+- *(2) BroadcastHashJoin [ID#57], [id#193], Inner, BuildRight\n",
      "   :- *(2) Project [ID#57, Name#58, Fee#59, Venue#60, Date#61, Duration#62]\n",
      "   :  +- *(2) Filter isnotnull(ID#57)\n",
      "   :     +- *(2) FileScan csv [ID#57,Name#58,Fee#59,Venue#60,Date#61,Duration#62] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/mnt/c/projects/notebook/CRT020/DataDummy/data/DataBricks_Training.csv], PartitionFilters: [], PushedFilters: [IsNotNull(ID)], ReadSchema: struct<ID:int,Name:string,Fee:int,Venue:string,Date:string,Duration:int>\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n",
      "      +- *(1) Project [id#193, end_date#194, start_date#195, location#196]\n",
      "         +- *(1) Filter isnotnull(id#193)\n",
      "            +- *(1) FileScan csv [id#193,end_date#194,start_date#195,location#196] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/mnt/c/projects/notebook/CRT020/DataDummy/dataframe_sample.csv], PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:int,end_date:timestamp,start_date:timestamp,location:string>\n"
     ]
    }
   ],
   "source": [
    "training_df.join(training_double_df.hint(\"id\"), \"ID\").explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10485760\n"
     ]
    }
   ],
   "source": [
    "print(spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

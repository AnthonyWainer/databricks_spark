{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrame FAQs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.session import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"jdbc data sources\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- end_date: string (nullable = true)\n",
      " |-- start_date: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      "\n",
      "+---+-------------------+-------------------+--------+\n",
      "| id|           end_date|         start_date|location|\n",
      "+---+-------------------+-------------------+--------+\n",
      "|  1|2015-10-14 00:00:00|2015-09-14 00:00:00|   CA-SF|\n",
      "|  2|2015-10-15 01:00:20|2015-08-14 00:00:00|   CA-SD|\n",
      "|  3|2015-10-16 02:30:00|2015-01-14 00:00:00|   NY-NY|\n",
      "|  4|2015-10-17 03:00:20|2015-02-14 00:00:00|   NY-NY|\n",
      "|  5|2015-10-18 04:30:00|2014-04-14 00:00:00|   CA-SD|\n",
      "+---+-------------------+-------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\").options(header='true', delimiter = '|').load(\"tmp/dataframe_sample.csv\")\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of registering a UDF, call the builtin functions to perform operations on the columns.\n",
    "# This will provide a performance improvement as the builtins compile and run in the platform's JVM.\n",
    "\n",
    "# Convert to a Date type\n",
    "df = df.withColumn('date', F.to_date(df.end_date))\n",
    "\n",
    "# Parse out the date only\n",
    "df = df.withColumn('date_only', F.regexp_replace(df.end_date,' (\\d+)[:](\\d+)[:](\\d+).*$', ''))\n",
    "\n",
    "# Split a string and index a field\n",
    "df = df.withColumn('city', F.split(df.location, '-')[1])\n",
    "\n",
    "# Perform a date diff function\n",
    "df = df.withColumn('date_diff', F.datediff(F.to_date(df.end_date), F.to_date(df.start_date)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, end_date: string, start_date: string, location: string, date: date, date_only: string, city: string, date_diff: int]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+-------------------+--------+----------+----------+----+---------+\n",
      "| id|           end_date|         start_date|location|      date| date_only|city|date_diff|\n",
      "+---+-------------------+-------------------+--------+----------+----------+----+---------+\n",
      "|  1|2015-10-14 00:00:00|2015-09-14 00:00:00|   CA-SF|2015-10-14|2015-10-14|  SF|       30|\n",
      "|  2|2015-10-15 01:00:20|2015-08-14 00:00:00|   CA-SD|2015-10-15|2015-10-15|  SD|       62|\n",
      "|  3|2015-10-16 02:30:00|2015-01-14 00:00:00|   NY-NY|2015-10-16|2015-10-16|  NY|      275|\n",
      "|  4|2015-10-17 03:00:20|2015-02-14 00:00:00|   NY-NY|2015-10-17|2015-10-17|  NY|      245|\n",
      "|  5|2015-10-18 04:30:00|2014-04-14 00:00:00|   CA-SD|2015-10-18|2015-10-18|  SD|      552|\n",
      "+---+-------------------+-------------------+--------+----------+----------+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.registerTempTable(\"sample_df\")\n",
    "display(spark.sql(\"select * from sample_df\"))\n",
    "spark.sql(\"select * from sample_df\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I want to convert the DataFrame back to JSON strings to send back to Kafka."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{\"id\":\"1\",\"end_date\":\"2015-10-14 00:00:00\",\"start_date\":\"2015-09-14 00:00:00\",\"location\":\"CA-SF\",\"date\":\"2015-10-14\",\"date_only\":\"2015-10-14\",\"city\":\"SF\",\"date_diff\":30}',\n",
       " '{\"id\":\"2\",\"end_date\":\"2015-10-15 01:00:20\",\"start_date\":\"2015-08-14 00:00:00\",\"location\":\"CA-SD\",\"date\":\"2015-10-15\",\"date_only\":\"2015-10-15\",\"city\":\"SD\",\"date_diff\":62}']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#There is an underlying toJSON() function that returns an RDD of JSON strings using the column\n",
    "#names and schema to produce the JSON records.\n",
    "rdd_json = df.toJSON()\n",
    "rdd_json.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My UDF takes a parameter including the column to operate on. How do I pass this parameter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, end_date: string, start_date: string, location: string, date: date, date_only: string, city: string, date_diff: int, id_offset: int]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+-------------------+--------+----------+----------+----+---------+---------+\n",
      "| id|           end_date|         start_date|location|      date| date_only|city|date_diff|id_offset|\n",
      "+---+-------------------+-------------------+--------+----------+----------+----+---------+---------+\n",
      "|  1|2015-10-14 00:00:00|2015-09-14 00:00:00|   CA-SF|2015-10-14|2015-10-14|  SF|       30|     1001|\n",
      "|  2|2015-10-15 01:00:20|2015-08-14 00:00:00|   CA-SD|2015-10-15|2015-10-15|  SD|       62|     1002|\n",
      "|  3|2015-10-16 02:30:00|2015-01-14 00:00:00|   NY-NY|2015-10-16|2015-10-16|  NY|      275|     1003|\n",
      "|  4|2015-10-17 03:00:20|2015-02-14 00:00:00|   NY-NY|2015-10-17|2015-10-17|  NY|      245|     1004|\n",
      "|  5|2015-10-18 04:30:00|2014-04-14 00:00:00|   CA-SD|2015-10-18|2015-10-18|  SD|      552|     1005|\n",
      "+---+-------------------+-------------------+--------+----------+----------+----+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "#There is a function available called lit() that creates a constant column.\n",
    "add_n = udf(lambda x, y: x + y, IntegerType())\n",
    "\n",
    "# We register a UDF that adds a column to the DataFrame, and we cast the id column to an Integer type.\n",
    "df = df.withColumn('id_offset', add_n(F.lit(1000), df.id.cast(IntegerType())))\n",
    "display(df)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, end_date: string, start_date: string, location: string, date: date, date_only: string, city: string, date_diff: int, id_offset: int]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+-------------------+--------+----------+----------+----+---------+---------+\n",
      "| id|           end_date|         start_date|location|      date| date_only|city|date_diff|id_offset|\n",
      "+---+-------------------+-------------------+--------+----------+----------+----+---------+---------+\n",
      "|  1|2015-10-14 00:00:00|2015-09-14 00:00:00|   CA-SF|2015-10-14|2015-10-14|  SF|       30|     1001|\n",
      "|  2|2015-10-15 01:00:20|2015-08-14 00:00:00|   CA-SD|2015-10-15|2015-10-15|  SD|       62|     1002|\n",
      "+---+-------------------+-------------------+--------+----------+----------+----+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# any constants used by UDF will automatically pass through to workers\n",
    "N = 90\n",
    "last_n_days = udf(lambda x: x < N, BooleanType())\n",
    "\n",
    "df_filtered = df.filter(last_n_days(df.date_diff))\n",
    "display(df_filtered)\n",
    "df_filtered.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I have a table in the Hive metastore and I’d like to access to table as a DataFrame. \n",
    "### What’s the best way to define this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are multiple ways to define a DataFrame from a registered table. Syntax show below.\n",
    "# Call table(tableName) or select and filter specific columns using an SQL query.\n",
    "# Both return DataFrame types\n",
    "df_1 = spark.table(\"sample_df\")\n",
    "df_2 = spark.sql(\"select * from sample_df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I’d like to clear all the cached tables on the current cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There’s an API available to do this at a global level or per table.\n",
    "spark.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I’d like to compute aggregates on columns. What’s the best way to do this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[location: string, min(id): string, count(id): bigint, avg(date_diff): double]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+---------+--------------+\n",
      "|location|min(id)|count(id)|avg(date_diff)|\n",
      "+--------+-------+---------+--------------+\n",
      "|   NY-NY|      3|        2|         260.0|\n",
      "|   CA-SF|      1|        1|          30.0|\n",
      "|   CA-SD|      2|        2|         307.0|\n",
      "+--------+-------+---------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#There’s an API named agg(*exprs) that takes a list of column names and expressions for the type \n",
    "#of aggregation you’d like to compute. Documentation is available here. \n",
    "#You can leverage the built-in functions that mentioned above as part of the expressions for each column.\n",
    "# Provide the min, count, and avg and groupBy the location column. Diplay the results\n",
    "agg_df = df.groupBy(\"location\").agg(F.min(\"id\"), F.count(\"id\"), F.avg(\"date_diff\"))\n",
    "display(agg_df)\n",
    "agg_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I’d like to write out the DataFrames to Parquet, but would like to partition on a particular column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- end_date: string (nullable = true)\n",
      " |-- start_date: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- date_only: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- date_diff: integer (nullable = true)\n",
      " |-- id_offset: integer (nullable = true)\n",
      " |-- end_month: integer (nullable = true)\n",
      " |-- end_year: integer (nullable = true)\n",
      "\n",
      "+---+-------------------+-------------------+--------+----------+----------+----+---------+---------+---------+--------+\n",
      "| id|           end_date|         start_date|location|      date| date_only|city|date_diff|id_offset|end_month|end_year|\n",
      "+---+-------------------+-------------------+--------+----------+----------+----+---------+---------+---------+--------+\n",
      "|  1|2015-10-14 00:00:00|2015-09-14 00:00:00|   CA-SF|2015-10-14|2015-10-14|  SF|       30|     1001|       10|    2015|\n",
      "|  2|2015-10-15 01:00:20|2015-08-14 00:00:00|   CA-SD|2015-10-15|2015-10-15|  SD|       62|     1002|       10|    2015|\n",
      "|  3|2015-10-16 02:30:00|2015-01-14 00:00:00|   NY-NY|2015-10-16|2015-10-16|  NY|      275|     1003|       10|    2015|\n",
      "|  4|2015-10-17 03:00:20|2015-02-14 00:00:00|   NY-NY|2015-10-17|2015-10-17|  NY|      245|     1004|       10|    2015|\n",
      "|  5|2015-10-18 04:30:00|2014-04-14 00:00:00|   CA-SD|2015-10-18|2015-10-18|  SD|      552|     1005|       10|    2015|\n",
      "+---+-------------------+-------------------+--------+----------+----------+----+---------+---------+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#You can use the following APIs to accomplish this. Ensure the code does not create a large number \n",
    "#of partition columns with the datasets otherwise the overhead of the metadata can cause significant slow downs. \n",
    "#If there is a SQL table back by this directory, you will need to call refresh table <table-name> to update \n",
    "#the metadata prior to the query.\n",
    "df = df.withColumn('end_month', F.month('end_date'))\n",
    "df = df.withColumn('end_year', F.year('end_date'))\n",
    "df.write.partitionBy(\"end_year\", \"end_month\").mode(\"overwrite\").parquet(\"tmp/sample_table\")\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do I properly handle cases where I want to filter out NULL data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[col1: string, col2: int]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|col1|col2|\n",
      "+----+----+\n",
      "|test|   1|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# You can use filter() and provide similar syntax as you would with a SQL query.\n",
    "null_item_schema = StructType([StructField(\"col1\", StringType(), True),\n",
    "                               StructField(\"col2\", IntegerType(), True)])\n",
    "null_df = spark.createDataFrame([(\"test\", 1), (None, 2)], null_item_schema)\n",
    "display(null_df.filter(\"col1 IS NOT NULL\"))\n",
    "null_df.filter(\"col1 IS NOT NULL\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
